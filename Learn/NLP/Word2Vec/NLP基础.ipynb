{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## Aaconda 更新\n",
    "   Open Anaconda Prompt, **run as admin**\n",
    "   > conda update --all\n",
    "   \n",
    "   ## NLTK 安装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.set_proxy('http://127.0.0.1:1080', ('USERNAME', 'PASSWORD'))\n",
    "nltk.set_proxy('http://proxy:8080', ('USERNAME', 'PASSWORD'))\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 或者，直接指定下载哪个\n",
    "# nltk.download('popular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "会弹出来一个窗口：\n",
    "- Collections\n",
    "- Corpora 语料\n",
    "- Models 模型\n",
    "- All Packages\n",
    "\n",
    "## NLTK 自带语料库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
      "57340\n",
      "1161192\n"
     ]
    }
   ],
   "source": [
    "# 如果之前没有下载过 brown 语料库，可以临时下载\n",
    "# nltk.download('brown')\n",
    "\n",
    "# brown大学的语料库，包含很多分类\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# 如果遇到 \"No Disc\" error\n",
    "# Navigate to: HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\Windows\\ \n",
    "# Change the value of the ErrorMode key to 2.\n",
    "\n",
    "# 看看包含多少分类\n",
    "print(brown.categories())\n",
    "\n",
    "# 看看多少句子，多少单词\n",
    "print(len(brown.sents()))\n",
    "print(len(brown.words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本处理流程\n",
    "- Preprocess 预处理\n",
    "  - 包含 Tokenize 分词，等等其它步骤\n",
    "- 生成 Features 特征 (as X)\n",
    "- 机器学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize\n",
    "把长句拆成有意义的小部件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', ',', 'world']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "sentence = \"hello, world\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中英文NLP区别\n",
    "例句：今天/天气/不错\n",
    "- 启发式 Heuristic，寻找最长的拟合词\n",
    "- 机器学习/统计方法：HMM（隐马尔科夫链），CRF\n",
    "  - 斯坦福的 CoreNLP 可以支持中文\n",
    "  \n",
    "## 中文分词\n",
    "- 安装jieba:\n",
    "\n",
    "Open Anaconda Prompt\n",
    "> pip install --proxy 127.0.0.1:1080 jieba\n",
    "\n",
    "or\n",
    "> pip install jieba\n",
    "\n",
    "也可以研究下CoreNLP，也有中文分词功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache D:\\Temp\\jieba.cache\n",
      "Loading model cost 0.830 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=True) # 全模式\n",
    "\"/ \".join(seg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我/ 来到/ 北京/ 清华大学'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=False) # 精确模式\n",
    "\"/ \".join(seg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'他/ 来到/ 了/ 网易/ 杭研/ 大厦'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_list = jieba.cut(\"他来到了网易杭研大厦\") # 默认精确模式\n",
    "\"/ \".join(seg_list)\n",
    "# 注意：这里“杭研”并没有在词典中，但也被Viterbi算法识别出来了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'小明/ 硕士/ 毕业/ 于/ 中国/ 科学/ 学院/ 科学院/ 中国科学院/ 计算/ 计算所/ ，/ 后/ 在/ 日本/ 京都/ 大学/ 日本京都大学/ 深造'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_list = jieba.cut_for_search(\"小明硕士毕业于中国科学院计算所，后在日本京都大学深造\") # 搜索引擎模式\n",
    "\"/ \".join(seg_list)\n",
    "# 该模式所有可能的都排列出来了，更适合于搜索引擎的使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 有时候分词没有那么简单\n",
    "例如：\n",
    "- RT是转发\n",
    "- @\n",
    "- :) 笑脸\n",
    "- 等等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', '@', 'angelababy', ':', 'love', 'you', 'baby', '!', ':', 'D', 'http', ':', '//ah.love', '#', '168cm']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tweet = 'RT @angelababy: love you baby! :D http://ah.love #168cm'\n",
    "print(word_tokenize(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 社交语言的 tokenize\n",
    "\n",
    "对于下面的`emoticons_str`，例子：\n",
    "- `:-)`\n",
    "- `:)`\n",
    "\n",
    "正则对照表: http://www.regexlab.com/zh/regref.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # 眼睛\n",
    "        [oO\\-]? # 鼻子\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # 嘴\n",
    "    )\"\"\"\n",
    "\n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # at 某人\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # 话题标签\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # 数字\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # 含有 - 和 ‘ 的单词，例如 don't\n",
    "    r'(?:[\\w_]+)', # 其他\n",
    "    r'(?:\\S)' # 其他\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', '@angelababy', ':', 'love', 'you', 'baby', '!', ':D', 'http://ah.love', '#168cm']\n"
     ]
    }
   ],
   "source": [
    "tokens_re = re.compile(r'(' + '|'.join(regex_str) + ')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^' + emoticons_str + '$', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    "\n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase: # 对于表情，需要让“是否小写”这个参数不生效\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "tweet = 'RT @angelababy: love you baby! :D http://ah.love #168cm'\n",
    "print(preprocess(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这些特殊符号是否保留，需要根据你的任务来判断\n",
    "\n",
    "## 纷繁复杂的词形（英文）\n",
    "\n",
    "对于英文，从语素构成单词的方法主要有两大类(可能部分交叉)：inflection(屈折)和 derivation(派生)\n",
    "- Infection (屈折)\n",
    "  - 不影响词性 walk => walking => walked\n",
    "\n",
    "- Derivation\n",
    "  - 影响词性 nation => national => nationalize\n",
    "\n",
    "简单一点，对于英文，直接依赖 `word_tokenize`，寄希望语料库足够大。但是为了效果更好，需要更好的预处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词形归一化（英文）\n",
    "\n",
    "- Stemming 词⼲提取，⼀般来说，就是把不影响词性的inflection的⼩尾巴砍掉，例如：\n",
    "  - walking => walk\n",
    "  - walked => walk\n",
    "\n",
    "- Lemmatization 词形归⼀：把各种类型的词的变形，都归为⼀个形式。通过 wordnet 的网络，一个语料库，实现\n",
    "  - went => go\n",
    "  - are => be\n",
    "  \n",
    "## NLTK实现Stemming\n",
    "NLTK提供了不同的词干提取的类，各自有自己的规则，偷懒的可以用 SnowBallStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum\n",
      "presum\n",
      "multipli\n",
      "provis\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "print(porter_stemmer.stem('maximum'))\n",
    "print(porter_stemmer.stem('presumably'))\n",
    "print(porter_stemmer.stem('multiply'))\n",
    "print(porter_stemmer.stem('provision'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxim\n",
      "presum\n",
      "multiply\n",
      "provid\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "print(lancaster_stemmer.stem('maximum'))\n",
    "print(lancaster_stemmer.stem('presumably'))\n",
    "print(lancaster_stemmer.stem('multiply'))\n",
    "print(lancaster_stemmer.stem('provision'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum\n",
      "presum\n",
      "multipli\n",
      "provis\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "print(snowball_stemmer.stem('maximum'))\n",
    "print(snowball_stemmer.stem('presumably'))\n",
    "print(snowball_stemmer.stem('multiply'))\n",
    "print(snowball_stemmer.stem('provision'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK实现Lemma（词形归一）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n",
      "church\n",
      "aardwolf\n",
      "abacus\n",
      "hardrock\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "print(wordnet_lemmatizer.lemmatize('dogs'))\n",
    "print(wordnet_lemmatizer.lemmatize('churches'))\n",
    "print(wordnet_lemmatizer.lemmatize('aardwolves'))\n",
    "print(wordnet_lemmatizer.lemmatize('abaci'))\n",
    "print(wordnet_lemmatizer.lemmatize('hardrock'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemma的⼩问题\n",
    "\n",
    "例如，went是go的过去式，但也同时是个英文名。\n",
    "这里需要我们来告诉算法词性。\n",
    "\n",
    "**POS**: part of speech，即：在我的这句话中，词性是什么"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "went\n",
      "went\n",
      "go\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "print(wordnet_lemmatizer.lemmatize('went')) #默认参数是 'n'\n",
    "print(wordnet_lemmatizer.lemmatize('went', pos='n'))\n",
    "print(wordnet_lemmatizer.lemmatize('went', pos='v'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are\n",
      "is\n",
      "be\n",
      "be\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "print(wordnet_lemmatizer.lemmatize('are')) # are 作为名词，没有此单词，返回自己\n",
    "print(wordnet_lemmatizer.lemmatize('is')) # is 作为名词，没有此单词，返回自己\n",
    "\n",
    "print(wordnet_lemmatizer.lemmatize('are', pos='v'))\n",
    "print(wordnet_lemmatizer.lemmatize('is', pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK标注 POS Tag\n",
    "类似的，中文库中也有，例如jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'does', 'the', 'fox', 'say']\n",
      "[('what', 'WDT'), ('does', 'VBZ'), ('the', 'DT'), ('fox', 'NNS'), ('say', 'VBP')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text = nltk.word_tokenize('what does the fox say')\n",
    "print(text)\n",
    "print(nltk.pos_tag(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面词性的输出可以映射到简单的词性，例如 'NNS' => 'n'，提供给lemmatize函数进行归一化\n",
    "\n",
    "## Stopwords\n",
    "\n",
    "- 中文，例如 的,得,地\n",
    "- 英文，例如 the\n",
    "\n",
    "容易导致歧义\n",
    "\n",
    "英文停止词列表：http://www.ranks.nl/stopwords\n",
    "中文需要搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to D:\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 需要 先下载\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'good', 'guy']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "word_list = nltk.word_tokenize(\"He is a good guy\")\n",
    "filtered_words =  [word for word in word_list if word not in stopwords.words('english')]\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⼀条typical的⽂本预处理流⽔线\n",
    "\n",
    "- Raw_Text\n",
    "- Tokenize\n",
    "  - POS Tag\n",
    "- Lemma/Stemming\n",
    "- Stopwords\n",
    "- Word_List\n",
    "\n",
    "对于场景不同，上述步骤可能有所不同。\n",
    "- 例如，判断一个人的写作能力怎么样，或者判断文章的相似度，不能去除停止词，因为破坏了句子结构\n",
    "- 但是，加入用于 word2vec，用于判断词义，大家都去除停止词比较方便"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK在NLP上的经典应用\n",
    "\n",
    "- 情感分析 \n",
    "- ⽂本相似度 \n",
    "- ⽂本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 情感分析\n",
    "\n",
    "哪些是夸你，哪些是黑你\n",
    "\n",
    "最简单的是 sentiment dictionary，类似于关键词的打分机制\n",
    "- 简单，不需要机器学习\n",
    "- 应用相对广泛\n",
    "- ⽐如：AFINN-111 http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'good', 'book']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_dictionary = {}\n",
    "for line in open('data/AFINN-111.txt'):\n",
    "    word, score = line.split('\\t')\n",
    "    sentiment_dictionary[word] = int(score)\n",
    "\n",
    "words = nltk.word_tokenize(\"This is a good book\")\n",
    "words =  [word for word in words if word not in stopwords.words('english')]\n",
    "print(words)\n",
    "\n",
    "# 把这个打分表记录在⼀个Dict上以后\n",
    "# 跑一遍整个句⼦，把对应的值相加\n",
    "total_score = sum(sentiment_dictionary.get(word, 0) for word in words)\n",
    "# 有值就是Dict中的值，没有就是0\n",
    "# 于是你就得到了⼀个 sentiment score\n",
    "total_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该方法的问题：\n",
    "- 新词怎么办\n",
    "- 特殊词怎么办\n",
    "- 更深层次的玩意儿怎么办\n",
    "\n",
    "## 配上ML的情感分析\n",
    "\n",
    "下面的例子可以这样理解，\n",
    "- 有一组单词：\n",
    "`['this', 'is', 'a', 'good', 'awesome', 'bad', 'terrible', 'book']`。\n",
    "- 预处理后输入的其实把一句话中用一个向量表示。例如：\n",
    "`['this': True, 'is': True, 'a': True, 'good': True, 'awesome': False, 'bad': False, 'terrible': False, 'book': True]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "# 随⼿手造点训练集\n",
    "s1 = 'this is a good book'\n",
    "s2 = 'this is a awesome book'\n",
    "s3 = 'this is a bad book'\n",
    "s4 = 'this is a terrible book'\n",
    "\n",
    "def preprocess(s):\n",
    "    # Func: 句⼦处理\n",
    "    # 这⾥简单的⽤了split(), 把句子中每个单词分开\n",
    "    # 显然 还有更多的processing method可以用\n",
    "    return {word: True for word in s.lower().split()}\n",
    "    # return⻓这样:\n",
    "    # {'this': True, 'is':True, 'a':True, 'good':True, 'book':True}\n",
    "    # 其中, 前⼀个叫fname, 对应每个出现的⽂本单词;\n",
    "    # 后⼀个叫fval, 指的是每个⽂本单词对应的值。\n",
    "    # 这⾥我们⽤最简单的True,来表示,这个词『出现在当前的句子中』的意义。\n",
    "    # 当然啦, 我们以后可以升级这个⽅程, 让它带有更加⽜逼的fval, ⽐如 word2vec\n",
    "\n",
    "# 把训练集给做成标准形式\n",
    "training_data = [[preprocess(s1), 'pos'],\n",
    "                 [preprocess(s2), 'pos'],\n",
    "                 [preprocess(s3), 'neg'],\n",
    "                 [preprocess(s4), 'neg']]\n",
    "# 喂给model吃\n",
    "model = NaiveBayesClassifier.train(training_data)\n",
    "# 打出结果\n",
    "print(model.classify(preprocess('this is a good book')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
