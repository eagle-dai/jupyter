{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## Aaconda 更新\n",
    "   Open Anaconda Prompt, **run as admin**\n",
    "   > conda update --all\n",
    "   \n",
    "   ## NLTK 安装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.set_proxy('http://127.0.0.1:1080', ('USERNAME', 'PASSWORD'))\n",
    "nltk.set_proxy('http://proxy:8080', ('USERNAME', 'PASSWORD'))\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 或者，直接指定下载哪个\n",
    "# nltk.download('popular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "会弹出来一个窗口：\n",
    "- Collections\n",
    "- Corpora 语料\n",
    "- Models 模型\n",
    "- All Packages\n",
    "\n",
    "## NLTK 自带语料库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
      "57340\n",
      "1161192\n"
     ]
    }
   ],
   "source": [
    "# 如果之前没有下载过 brown 语料库，可以临时下载\n",
    "# nltk.download('brown')\n",
    "\n",
    "# brown大学的语料库，包含很多分类\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# 如果遇到 \"No Disc\" error\n",
    "# Navigate to: HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\Windows\\ \n",
    "# Change the value of the ErrorMode key to 2.\n",
    "\n",
    "# 看看包含多少分类\n",
    "print(brown.categories())\n",
    "\n",
    "# 看看多少句子，多少单词\n",
    "print(len(brown.sents()))\n",
    "print(len(brown.words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本处理流程\n",
    "- Preprocess 预处理\n",
    "  - 包含 Tokenize 分词，等等其它步骤\n",
    "- 生成 Features 特征 (as X)\n",
    "- 机器学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize\n",
    "把长句拆成有意义的小部件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', ',', 'world']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "sentence = \"hello, world\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中英文NLP区别\n",
    "例句：今天/天气/不错\n",
    "- 启发式 Heuristic，寻找最长的拟合词\n",
    "- 机器学习/统计方法：HMM（隐马尔科夫链），CRF\n",
    "  - 斯坦福的 CoreNLP 可以支持中文\n",
    "  \n",
    "## 中文分词\n",
    "- 安装jieba:\n",
    "\n",
    "Open Anaconda Prompt\n",
    "> pip install --proxy 127.0.0.1:1080 jieba\n",
    "\n",
    "or\n",
    "> pip install jieba\n",
    "\n",
    "也可以研究下CoreNLP，也有中文分词功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache D:\\Temp\\jieba.cache\n",
      "Loading model cost 0.934 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=True) # 全模式\n",
    "\"/ \".join(seg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我/ 来到/ 北京/ 清华大学'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=False) # 精确模式\n",
    "\"/ \".join(seg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'他/ 来到/ 了/ 网易/ 杭研/ 大厦'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_list = jieba.cut(\"他来到了网易杭研大厦\") # 默认精确模式\n",
    "\"/ \".join(seg_list)\n",
    "# 注意：这里“杭研”并没有在词典中，但也被Viterbi算法识别出来了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'小明/ 硕士/ 毕业/ 于/ 中国/ 科学/ 学院/ 科学院/ 中国科学院/ 计算/ 计算所/ ，/ 后/ 在/ 日本/ 京都/ 大学/ 日本京都大学/ 深造'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_list = jieba.cut_for_search(\"小明硕士毕业于中国科学院计算所，后在日本京都大学深造\") # 搜索引擎模式\n",
    "\"/ \".join(seg_list)\n",
    "# 该模式所有可能的都排列出来了，更适合于搜索引擎的使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 有时候分词没有那么简单\n",
    "例如：\n",
    "- RT是转发\n",
    "- @\n",
    "- :) 笑脸\n",
    "- 等等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', '@', 'angelababy', ':', 'love', 'you', 'baby', '!', ':', 'D', 'http', ':', '//ah.love', '#', '168cm']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tweet = 'RT @angelababy: love you baby! :D http://ah.love #168cm'\n",
    "print(word_tokenize(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 社交语言的 tokenize\n",
    "\n",
    "对于下面的`emoticons_str`，例子：\n",
    "- `:-)`\n",
    "- `:)`\n",
    "\n",
    "正则对照表: http://www.regexlab.com/zh/regref.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # 眼睛\n",
    "        [oO\\-]? # 鼻子\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # 嘴\n",
    "    )\"\"\"\n",
    "\n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # at 某人\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # 话题标签\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # 数字\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # 含有 - 和 ‘ 的单词，例如 don't\n",
    "    r'(?:[\\w_]+)', # 其他\n",
    "    r'(?:\\S)' # 其他\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', '@angelababy', ':', 'love', 'you', 'baby', '!', ':D', 'http://ah.love', '#168cm']\n"
     ]
    }
   ],
   "source": [
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    "\n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "tweet = 'RT @angelababy: love you baby! :D http://ah.love #168cm'\n",
    "print(preprocess(tweet))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
