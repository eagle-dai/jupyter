{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## Aaconda 更新\n",
    "   Open Anaconda Prompt, **run as admin**\n",
    "   > conda update --all\n",
    "   \n",
    "   ## NLTK 安装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.set_proxy('http://127.0.0.1:1080', ('USERNAME', 'PASSWORD'))\n",
    "nltk.set_proxy('http://proxy:8080', ('USERNAME', 'PASSWORD'))\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 或者，直接指定下载哪个\n",
    "# nltk.download('popular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "会弹出来一个窗口：\n",
    "- Collections\n",
    "- Corpora 语料\n",
    "- Models 模型\n",
    "- All Packages\n",
    "\n",
    "## NLTK 自带语料库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
      "57340\n",
      "1161192\n"
     ]
    }
   ],
   "source": [
    "# 如果之前没有下载过 brown 语料库，可以临时下载\n",
    "# nltk.download('brown')\n",
    "\n",
    "# brown大学的语料库，包含很多分类\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# 如果遇到 \"No Disc\" error\n",
    "# Navigate to: HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\Windows\\ \n",
    "# Change the value of the ErrorMode key to 2.\n",
    "\n",
    "# 看看包含多少分类\n",
    "print(brown.categories())\n",
    "\n",
    "# 看看多少句子，多少单词\n",
    "print(len(brown.sents()))\n",
    "print(len(brown.words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本处理流程\n",
    "- Preprocess 预处理\n",
    "  - 包含 Tokenize 分词，等等其它步骤\n",
    "- 生成 Features 特征 (as X)\n",
    "- 机器学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize\n",
    "把长句拆成有意义的小部件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', ',', 'world']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "sentence = \"hello, world\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中英文NLP区别\n",
    "例句：今天/天气/不错\n",
    "- 启发式 Heuristic，寻找最长的拟合词\n",
    "- 机器学习/统计方法：HMM（隐马尔科夫链），CRF\n",
    "  - 斯坦福的 CoreNLP 可以支持中文\n",
    "  \n",
    "## 中文分词\n",
    "- 安装jieba:\n",
    "\n",
    "Open Anaconda Prompt\n",
    "> pip install --proxy 127.0.0.1:1080 jieba\n",
    "\n",
    "or\n",
    "> pip install jieba\n",
    "\n",
    "也可以研究下CoreNLP，也有中文分词功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache G:\\TEMP\\jieba.cache\n",
      "Loading model cost 0.798 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=True) # 全模式\n",
    "\"/ \".join(seg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我/ 来到/ 北京/ 清华大学'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=False) # 精确模式\n",
    "\"/ \".join(seg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'他/ 来到/ 了/ 网易/ 杭研/ 大厦'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_list = jieba.cut(\"他来到了网易杭研大厦\") # 默认精确模式\n",
    "\"/ \".join(seg_list)\n",
    "# 注意：这里“杭研”并没有在词典中，但也被Viterbi算法识别出来了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'小明/ 硕士/ 毕业/ 于/ 中国/ 科学/ 学院/ 科学院/ 中国科学院/ 计算/ 计算所/ ，/ 后/ 在/ 日本/ 京都/ 大学/ 日本京都大学/ 深造'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_list = jieba.cut_for_search(\"小明硕士毕业于中国科学院计算所，后在日本京都大学深造\") # 搜索引擎模式\n",
    "\"/ \".join(seg_list)\n",
    "# 该模式所有可能的都排列出来了，更适合于搜索引擎的使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 有时候分词没有那么简单\n",
    "例如：\n",
    "- RT是转发\n",
    "- @\n",
    "- :) 笑脸\n",
    "- 等等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', '@', 'angelababy', ':', 'love', 'you', 'baby', '!', ':', 'D', 'http', ':', '//ah.love', '#', '168cm']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tweet = 'RT @angelababy: love you baby! :D http://ah.love #168cm'\n",
    "print(word_tokenize(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 社交语言的 tokenize\n",
    "\n",
    "对于下面的`emoticons_str`，例子：\n",
    "- `:-)`\n",
    "- `:)`\n",
    "\n",
    "正则对照表: http://www.regexlab.com/zh/regref.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # 眼睛\n",
    "        [oO\\-]? # 鼻子\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # 嘴\n",
    "    )\"\"\"\n",
    "\n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # at 某人\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # 话题标签\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # 数字\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # 含有 - 和 ‘ 的单词，例如 don't\n",
    "    r'(?:[\\w_]+)', # 其他\n",
    "    r'(?:\\S)' # 其他\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', '@angelababy', ':', 'love', 'you', 'baby', '!', ':D', 'http://ah.love', '#168cm']\n"
     ]
    }
   ],
   "source": [
    "tokens_re = re.compile(r'(' + '|'.join(regex_str) + ')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^' + emoticons_str + '$', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    "\n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase: # 对于表情，需要让“是否小写”这个参数不生效\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "tweet = 'RT @angelababy: love you baby! :D http://ah.love #168cm'\n",
    "print(preprocess(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这些特殊符号是否保留，需要根据你的任务来判断\n",
    "\n",
    "## 纷繁复杂的词形（英文）\n",
    "\n",
    "对于英文，从语素构成单词的方法主要有两大类(可能部分交叉)：inflection(屈折)和 derivation(派生)\n",
    "- Infection (屈折)\n",
    "  - 不影响词性 walk => walking => walked\n",
    "\n",
    "- Derivation\n",
    "  - 影响词性 nation => national => nationalize\n",
    "\n",
    "简单一点，对于英文，直接依赖 `word_tokenize`，寄希望语料库足够大。但是为了效果更好，需要更好的预处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词形归一化（英文）\n",
    "\n",
    "- Stemming 词⼲提取，⼀般来说，就是把不影响词性的inflection的⼩尾巴砍掉，例如：\n",
    "  - walking => walk\n",
    "  - walked => walk\n",
    "\n",
    "- Lemmatization 词形归⼀：把各种类型的词的变形，都归为⼀个形式。通过 wordnet 的网络，一个语料库，实现\n",
    "  - went => go\n",
    "  - are => be\n",
    "  \n",
    "## NLTK实现Stemming\n",
    "NLTK提供了不同的词干提取的类，各自有自己的规则，偷懒的可以用 SnowBallStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum\n",
      "presum\n",
      "multipli\n",
      "provis\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "print(porter_stemmer.stem('maximum'))\n",
    "print(porter_stemmer.stem('presumably'))\n",
    "print(porter_stemmer.stem('multiply'))\n",
    "print(porter_stemmer.stem('provision'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxim\n",
      "presum\n",
      "multiply\n",
      "provid\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "print(lancaster_stemmer.stem('maximum'))\n",
    "print(lancaster_stemmer.stem('presumably'))\n",
    "print(lancaster_stemmer.stem('multiply'))\n",
    "print(lancaster_stemmer.stem('provision'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum\n",
      "presum\n",
      "multipli\n",
      "provis\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "print(snowball_stemmer.stem('maximum'))\n",
    "print(snowball_stemmer.stem('presumably'))\n",
    "print(snowball_stemmer.stem('multiply'))\n",
    "print(snowball_stemmer.stem('provision'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK实现Lemma（词形归一）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n",
      "church\n",
      "aardwolf\n",
      "abacus\n",
      "hardrock\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "print(wordnet_lemmatizer.lemmatize('dogs'))\n",
    "print(wordnet_lemmatizer.lemmatize('churches'))\n",
    "print(wordnet_lemmatizer.lemmatize('aardwolves'))\n",
    "print(wordnet_lemmatizer.lemmatize('abaci'))\n",
    "print(wordnet_lemmatizer.lemmatize('hardrock'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemma的小问题\n",
    "\n",
    "例如，went是go的过去式，但也同时是个英文名。\n",
    "这里需要我们来告诉算法词性。\n",
    "\n",
    "**POS**: part of speech，即：在我的这句话中，词性是什么"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "went\n",
      "went\n",
      "go\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "print(wordnet_lemmatizer.lemmatize('went')) #默认参数是 'n'\n",
    "print(wordnet_lemmatizer.lemmatize('went', pos='n'))\n",
    "print(wordnet_lemmatizer.lemmatize('went', pos='v'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are\n",
      "is\n",
      "be\n",
      "be\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "print(wordnet_lemmatizer.lemmatize('are')) # are 作为名词，没有此单词，返回自己\n",
    "print(wordnet_lemmatizer.lemmatize('is')) # is 作为名词，没有此单词，返回自己\n",
    "\n",
    "print(wordnet_lemmatizer.lemmatize('are', pos='v'))\n",
    "print(wordnet_lemmatizer.lemmatize('is', pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK标注 POS Tag\n",
    "类似的，中文库中也有，例如jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'does', 'the', 'fox', 'say']\n",
      "[('what', 'WDT'), ('does', 'VBZ'), ('the', 'DT'), ('fox', 'NNS'), ('say', 'VBP')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text = nltk.word_tokenize('what does the fox say')\n",
    "print(text)\n",
    "print(nltk.pos_tag(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面词性的输出可以映射到简单的词性，例如 'NNS' => 'n'，提供给lemmatize函数进行归一化\n",
    "\n",
    "## Stopwords\n",
    "\n",
    "- 中文，例如 的,得,地\n",
    "- 英文，例如 the\n",
    "\n",
    "容易导致歧义\n",
    "\n",
    "英文停止词列表：http://www.ranks.nl/stopwords\n",
    "中文需要搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11004]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 需要 先下载\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'good', 'guy']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "word_list = nltk.word_tokenize(\"He is a good guy\")\n",
    "filtered_words =  [word for word in word_list if word not in stopwords.words('english')]\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⼀条typical的⽂本预处理流⽔线\n",
    "\n",
    "- Raw_Text\n",
    "- Tokenize\n",
    "  - POS Tag\n",
    "- Lemma/Stemming\n",
    "- Stopwords\n",
    "- Word_List\n",
    "\n",
    "对于场景不同，上述步骤可能有所不同。\n",
    "- 例如，判断一个人的写作能力怎么样，或者判断文章的相似度，不能去除停止词，因为破坏了句子结构\n",
    "- 但是，加入用于 word2vec，用于判断词义，大家都去除停止词比较方便"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK在NLP上的经典应用\n",
    "\n",
    "- 情感分析 \n",
    "- ⽂本相似度 \n",
    "- ⽂本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 情感分析\n",
    "\n",
    "哪些是夸你，哪些是黑你\n",
    "\n",
    "最简单的是 sentiment dictionary，类似于关键词的打分机制\n",
    "- 简单，不需要机器学习\n",
    "- 应用相对广泛\n",
    "- ⽐如：AFINN-111 http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'good', 'book']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_dictionary = {}\n",
    "for line in open('data/AFINN-111.txt'):\n",
    "    word, score = line.split('\\t')\n",
    "    sentiment_dictionary[word] = int(score)\n",
    "\n",
    "words = nltk.word_tokenize(\"This is a good book\")\n",
    "words =  [word for word in words if word not in stopwords.words('english')]\n",
    "print(words)\n",
    "\n",
    "# 把这个打分表记录在个Dict上以后\n",
    "# 跑一遍整个句子，把对应的值相加\n",
    "total_score = sum(sentiment_dictionary.get(word, 0) for word in words)\n",
    "# 有值就是Dict中的值，没有就是0\n",
    "# 于是你就得到了一个 sentiment score\n",
    "total_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该方法的问题：\n",
    "- 新词怎么办\n",
    "- 特殊词怎么办\n",
    "- 更深层次的玩意儿怎么办\n",
    "\n",
    "## 配上ML的情感分析\n",
    "\n",
    "下面的例子可以这样理解，\n",
    "- 有一组单词：\n",
    "`['this', 'is', 'a', 'good', 'awesome', 'bad', 'terrible', 'book']`。\n",
    "- 预处理后输入的其实把一句话中用一个向量表示。例如：\n",
    "`['this': True, 'is': True, 'a': True, 'good': True, 'awesome': False, 'bad': False, 'terrible': False, 'book': True]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "# 随手造点训练集\n",
    "s1 = 'this is a good book'\n",
    "s2 = 'this is a awesome book'\n",
    "s3 = 'this is a bad book'\n",
    "s4 = 'this is a terrible book'\n",
    "\n",
    "def preprocess(s):\n",
    "    # Func: 句子处理\n",
    "    # 这里简单的用了split(), 把句子中每个单词分开\n",
    "    # 显然 还有更多的processing method可以用\n",
    "    return {word: True for word in s.lower().split()}\n",
    "    # return长这样:\n",
    "    # {'this': True, 'is':True, 'a':True, 'good':True, 'book':True}\n",
    "    # 其中, 前⼀个叫fname, 对应每个出现的文本单词;\n",
    "    # 后一个叫fval, 指的是每个⽂本单词对应的值。\n",
    "    # 这里我们用最简单的True,来表示,这个词『出现在当前的句子中』的意义。\n",
    "    # 当然啦, 我们以后可以升级这个方程, 让它带有更加牛逼的fval, 例如 word2vec\n",
    "\n",
    "# 把训练集给做成标准形式\n",
    "training_data = [[preprocess(s1), 'pos'],\n",
    "                 [preprocess(s2), 'pos'],\n",
    "                 [preprocess(s3), 'neg'],\n",
    "                 [preprocess(s4), 'neg']]\n",
    "# 喂给model吃\n",
    "model = NaiveBayesClassifier.train(training_data)\n",
    "# 打出结果\n",
    "print(model.classify(preprocess('this is a good book')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 应用：文本相似度\n",
    "\n",
    "### 用元素频率表示文本特征\n",
    "\n",
    "例如，有三句话，对应于三个特征：\n",
    "\n",
    "| we    | you   |  he  | work | happy | are |\n",
    "| -----  | -----  | ---- |-----  | -----  | ---- |\n",
    "|1|0|3|0|1|1|\n",
    "|1|0|2|0|1|1|\n",
    "|0|1|0|1|0|0|\n",
    "\n",
    "很可能对应的三句话是：\n",
    "- he he he, we are happy\n",
    "- he he, we are happy\n",
    "- you work\n",
    "\n",
    "第一句话对应的向量：\n",
    "- v1: `[1, 0, 3, 0, 1, 1]`\n",
    "- v2: `[1, 0, 2, 0, 1, 1]`\n",
    "- v3: `[0, 1, 0, 1, 0, 0]`\n",
    "\n",
    "这样的好处是，每个向量都等长\n",
    "\n",
    "### 余弦定理\n",
    "\n",
    "similarity = cos(theta) = (A . B) / (||A|| ||B||)\n",
    "\n",
    "## Frequency 频率统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'my', 'sentence', 'this', 'is', 'my', 'life', 'this', 'is', 'the', 'day']\n",
      "3 3 2 0\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "\n",
    "# 做个词库先\n",
    "corpus = 'this is my sentence ' \\\n",
    "            'this is my life ' \\\n",
    "            'this is the day'\n",
    "\n",
    "# 随便tokenize一下。\n",
    "# 显然，正如上文提到，这里可以根据需要做任何的 preprocessing:\n",
    "# stopwords, lemma, stemming, etc.\n",
    "tokens = nltk.word_tokenize(corpus)\n",
    "print(tokens)\n",
    "\n",
    "# 借用 NLTK 的 FreqDist 统计一下文字出现的频率\n",
    "fdist = FreqDist(tokens)\n",
    "\n",
    "# 它就类似于一个Dict\n",
    "print(fdist['this'], fdist['is'], fdist['my'], fdist['none'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 7\n",
      "[('this', 3), ('is', 3), ('my', 2), ('sentence', 1), ('life', 1), ('the', 1), ('day', 1)]\n"
     ]
    }
   ],
   "source": [
    "# 好，此刻，我们可以把最常用的50个单词拿出来\n",
    "standard_freq_vector = fdist.most_common(50)\n",
    "size = len(standard_freq_vector)\n",
    "print('size:', size)\n",
    "print(standard_freq_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 0, 'is': 1, 'my': 2, 'sentence': 3, 'life': 4, 'the': 5, 'day': 6}\n"
     ]
    }
   ],
   "source": [
    "# Func: 按照出现频率大小，记录下每一个单词的位置\n",
    "def position_lookup(v):\n",
    "    res = {}\n",
    "    counter = 0\n",
    "    for word in v:\n",
    "        res[word[0]] = counter\n",
    "        counter += 1\n",
    "    return res\n",
    "\n",
    "# 把标准的单词位置记录下来\n",
    "standard_position_dict = position_lookup(standard_freq_vector)\n",
    "# 得到一个位置对照表\n",
    "print(standard_position_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# 这时，我们有了个新句子，要对它的词频进行统计\n",
    "sentence = 'this is cool'\n",
    "# freq_vector 会作为最后的输出，先初始化\n",
    "freq_vector = [0] * size\n",
    "\n",
    "# 简单的 preprocessing\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "# 循环统计词频\n",
    "for word in tokens:\n",
    "    try:\n",
    "        freq_vector[standard_position_dict[word]] += 1\n",
    "    except KeyError:\n",
    "        continue\n",
    "    \n",
    "print(freq_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 应用：文本分类\n",
    "\n",
    "### IF-IDF (term frequency - inverse document frequency 词频-逆文本频率)\n",
    "\n",
    "\n",
    "是一种用于信息检索与文本挖掘的常用加权技术。tf-idf是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。\n",
    "- TF: 词频，衡量⼀个term在⽂档中出现得有多频繁\n",
    "- IDF: 衡量⼀个term本身有多重要。相当于新鲜程度。新鲜程度高，意味着普遍性低。\n",
    "\n",
    "概括来讲，IDF反应了一个词在所有文本中出现的频率，如果一个词在很多的文本中出现，那么它的IDF值应该低，比如“非常”。而反过来如果一个词在比较少的文本中出现，那么它的IDF值应该高。一个极端的情况，如果一个词在所有的文本中都出现，那么它的IDF值应该为0。(https://zhuanlan.zhihu.com/p/41613659)\n",
    "\n",
    "- TF计算，好理解，某词在某文档中的频率（比例）\n",
    "- IDF = log(文档总数 / (含有该词的文档总数 + 1))\n",
    "  - 这里+1用来平滑，防止分母为0的情况\n",
    "- TF-IDF = TF * IDF\n",
    "\n",
    "例如，\n",
    "- 某个文档有100个单词，其中baby出现3次，那么 TF(baby) = 3 / 100 = 0.03\n",
    "- 我们现在如果语料库中有10M个文档（1千万个文档），baby出现在其中的1000个文档中。那么 IDF(baby) = log(10,000,000 / 1001) = 4\n",
    "- 所以：TF-IDF(baby) = TF(baby) * IDF(baby) = 0.03 * 4 = 0.12\n",
    "\n",
    "## NLTK 实现 TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01930786229086497\n",
      "[0.01930786229086497, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "from nltk.text import TextCollection\n",
    "\n",
    "# 首先，把所有文档都放到 TextCollection 中。这个类可以断句，做统计，做计算\n",
    "corpus = TextCollection(['this is sentence one', \n",
    "                         'this is sentence two', \n",
    "                         'that is sentence three'])\n",
    "\n",
    "# 直接就能算出 TF-IDF\n",
    "# (term: 一句话中的某个 term， text: 这句话)\n",
    "print(corpus.tf_idf('this', 'this is sentence four'))\n",
    "\n",
    "# 同理，怎么得到一个标准大小的 vector 来表示所有的句子？\n",
    "# 对于每个新句子\n",
    "new_sentence = 'this is sentence five'\n",
    "# 遍历一遍所有 vocabulary 中的词：\n",
    "standard_vocab = ['this', 'is', 'that', 'sentence', 'one', 'two', 'three']\n",
    "                 # 这里，standard_vocab 是语料中出现过的所有的词\n",
    "output_vector = [0] * len(standard_vocab)\n",
    "i = 0\n",
    "for word in standard_vocab:\n",
    "    output_vector[i] = corpus.tf_idf(word, new_sentence)\n",
    "    i += 1\n",
    "print(output_vector)\n",
    "# 我们会得到一个巨长（=所有vocab长度）的向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 接下来: Vec => Label\n",
    "\n",
    "可能的ML模型\n",
    "- SVM\n",
    "- LR\n",
    "- RF: 随机森林\n",
    "- MLP: 多层神经网络(多层感知器)\n",
    "- LSTM\n",
    "- RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
